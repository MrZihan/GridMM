# GridMM: Grid Memory Map for Vision-and-Language Navigation

### Zihan Wang and Xiangyang Li and Jiahao Yang and Yeqi Liu and Shuqiang Jiang

This repository is the official implementation of **[GridMM: Grid Memory Map for Vision-and-Language Navigation].**

>Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method.

![image](https://github.com/MrZihan/GridMM/blob/main/demo.gif)

The following will be processed within a few days:
1) Upload code of GridMM in discrete environments(R2R,REVERIE,SOON). (Finished)
2) Upload code of data preprocessing. (In progress)
3) Upload feature files and checkpoints. (In progress)
4) Upload code of GridMM in continuous environments(R2R-CE). (In progress)
5) Upload the document. (In progress)
